// src/aiEngine.js
import { CreateMLCEngine } from "@mlc-ai/web-llm";

//const selectedModel = "Llama-3.2-1B-Instruct-q4f16_1-MLC";
//const selectedModel = "gemma-2b-it-q4f16_1-MLC";
const selectedModel = "TinyLlama-1.1B-Chat-v1.0-q4f16_1-MLC"; // ë§¤ìš° ê°€ë²¼ì›€ - TinyLlama
//const selectedModel = "Qwen2-0.5B-Instruct-q4f16_1-MLC"; //ê°€ì¥ ê°€ë²¼ì›€ - Qwen


let engine = null;
let isInitializing = false; // ì´ˆê¸°í™” ì¤‘ë³µ ë°©ì§€

export const initLLM = async (onProgress) => {
	if (engine) return engine;
	if (isInitializing) return;

	isInitializing = true;
	try {
		engine = await CreateMLCEngine(selectedModel, {
			initProgressCallback: onProgress,
		});
		isInitializing = false;
		return engine;
	} catch (error) {
		isInitializing = false;
		console.error("LLM ì´ˆê¸°í™” ì—ëŸ¬:", error);
		throw error;
	}
};

export const generateMusicFeedback = async (metrics) => {
	if (!engine || isInitializing) return "ë¶„ì„ ì¤‘...";

	// ğŸ’¡ ì˜ˆì‹œ ë¬¸ì¥ì„ ì‚­ì œí•˜ê³ , ì ìˆ˜ ë°ì´í„°ë¥¼ ì§ì ‘ì ìœ¼ë¡œ í•´ì„í•˜ë„ë¡ ì§€ì‹œí•©ë‹ˆë‹¤.
	const prompt = `
    ë‹¹ì‹ ì€ ìŒì•… ì½”ì¹˜ì…ë‹ˆë‹¤. ì•„ë˜ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ í•œêµ­ì–´ë¡œë§Œ ì¡°ì–¸í•˜ì„¸ìš”.
    ë°ì´í„°: ìŒì • ${metrics.pitch}%, ë¦¬ë“¬ ${metrics.rhythm}%, ìì„¸ ${metrics.technique}%
    `;

	try {
		const reply = await engine.chat.completions.create({
			messages: [
				{ role: "system", content: "í•œêµ­ì–´ ìŒì•… êµìœ¡ ì „ë¬¸ê°€ë¡œì„œ ê°„ê²°í•˜ê²Œ ë‹µë³€í•©ë‹ˆë‹¤." },
				{ role: "user", content: prompt }
			],
			temperature: 0.5, // 0.2ë³´ë‹¤ ì¡°ê¸ˆ ë†’ì—¬ì„œ ì˜ˆì‹œë¥¼ ë² ë¼ì§€ ì•Šê³  ë¬¸ì¥ì„ ìƒì„±í•˜ê²Œ í•©ë‹ˆë‹¤.
			max_tokens: 300,
		});

		return reply.choices[0].message.content;
	} catch (error) {
		console.error("í”¼ë“œë°± ìƒì„± ì‹¤íŒ¨:", error);
		return "AI ì—”ì§„ í†µì‹  ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.";
	}
};
